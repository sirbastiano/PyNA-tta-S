{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pynattas as pnas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datamodule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets.SPECTRE_prova.dataset as dataset_SPECTRE\n",
    "import os\n",
    "import json\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "import pytorch_lightning as pl\n",
    "from pycocotools.coco import COCO\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the data module with transformations\n",
    "data_module = dataset_SPECTRE.SPECTRE_COCO_DataModule(img_dir=rf'D:\\Andrea\\DicDic\\data\\SPECTRE_prova\\images', ann_file=rf'D:\\Andrea\\DicDic\\data\\SPECTRE_prova\\annotations_SPECTRE.json', batch_size=4, transform=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = data_module.train_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for images, targets in train_loader:\n",
    "    print(\"Batch Image Shape:\", images[0].shape)\n",
    "    print(\"Batch Targets:\", targets)\n",
    "    break  # Inspect only the first batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with rasterio.open(rf\"D:\\Andrea\\DicDic\\data\\SPECTRE_prova\\images\\DAN_S1A_20240301T054857_TC_Sub1_crop1_VH_636018753.tif\") as src:\n",
    "    image = src.read()  # Read the single channel\n",
    "    #image = np.expand_dims(image, axis=0)  # Add channel dimension to make it (height, width, 1)\n",
    "\n",
    "print(image.shape)\n",
    "#image = Image.fromarray(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = image[0,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(image, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.mean(image)\n",
    "std = np.std(image)\n",
    "mask = image > (mean + 7*std)\n",
    "\n",
    "plt.imshow(mask, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANCORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([[10, 25], \n",
    " [15, 15], \n",
    " [25, 10], \n",
    " [25, 50],\n",
    " [35, 35], \n",
    " [50, 25]])/512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing a single run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pynattas as pnas\n",
    "from torchvision import transforms\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "import torch\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from datasets.SPECTRE_prova.dataset import SPECTRE_COCO_Dataset, SPECTRE_COCO_DataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir='data/SPECTRE_prova'\n",
    "input_size = 512\n",
    "custom_transform = transforms.Compose([\n",
    "    #transforms.Resize((input_size, input_size)),  # Resize images to the size expected by your model\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "dm = SPECTRE_COCO_DataModule(\n",
    "        img_dir=root_dir + '/images',\n",
    "        ann_file=root_dir + '/annotations_SPECTRE.json',\n",
    "        batch_size=4,\n",
    "        num_workers=0,\n",
    "        transform=custom_transform,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This architecture has 3 encoder layers.\n",
      "Lne4agn1EPIELpe3aln1EPMELme3arn1EPaEHSu2v4EE\n",
      "[{'layer_type': 'MBConvNoRes', 'expansion_factor': 4, 'activation': 'GELU', 'num_blocks': 1}, {'layer_type': 'Identity'}, {'layer_type': 'CSPMBConvBlock', 'expansion_factor': 3, 'activation': 'LeakyReLU', 'num_blocks': 1}, {'layer_type': 'MaxPool'}, {'layer_type': 'MBConv', 'expansion_factor': 3, 'activation': 'ReLU', 'num_blocks': 1}, {'layer_type': 'AvgPool'}, {'layer_type': 'DetectionHeadYOLOv3_SmallObjects', 'outchannel1_index': 2, 'outchannel2_index': 4}]\n"
     ]
    }
   ],
   "source": [
    "code = pnas.functions.architecture_builder.generate_random_architecture_code(4)\n",
    "print(code)\n",
    "parsed_layers = pnas.functions.architecture_builder.parse_architecture_code(code)\n",
    "print(parsed_layers)\n",
    "\n",
    "model = pnas.classes.GenericOD_YOLOv3_SmallObjects(\n",
    "    parsed_layers=parsed_layers,\n",
    "    input_channels=1,\n",
    "    input_height=512,\n",
    "    input_width=512,\n",
    "    num_classes=1,\n",
    "    learning_rate=1e-2,\n",
    ")\n",
    "if model.model.is_too_deep:\n",
    "    print(\"This architecture is too deep and the tensor lost its dimensionality\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out1:  torch.Size([1, 3072, 6]) - out2:  torch.Size([1, 12288, 6]) - out:  torch.Size([1, 15360, 6])\n",
      "Output shape: torch.Size([1, 15360, 6])\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "# Generate a random tensor with the appropriate shape\n",
    "input_tensor = torch.randn(1, 1, 512, 512)  # Batch size of 1, 3 channels, 416x416 height and width\n",
    "\n",
    "# Pass the tensor through the model\n",
    "output = model(input_tensor)\n",
    "\n",
    "# Optional: Print the output shape to verify\n",
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(overfit_batches=1)` was configured so 1 batch will be used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\andre\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type           | Params\n",
      "-----------------------------------------\n",
      "0 | model | GenericNetwork | 5.0 M \n",
      "-----------------------------------------\n",
      "5.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "5.0 M     Total params\n",
      "20.131    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/True [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\andre\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out1:  torch.Size([4, 3072, 6]) - out2:  torch.Size([4, 12288, 6]) - out:  torch.Size([4, 15360, 6])\n",
      "Noobj_loss: 0.9716344475746155\n",
      "Coord_loss: 9.695586204528809\n",
      "Obj_loss: 0.4738689363002777\n",
      "Class_loss: 0\n",
      "                                                                              "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\andre\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 4. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Users\\andre\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\logger_connector\\result.py:212: You called `self.log('val_noobj_loss', ...)` in your `validation_step` but the value needs to be floating to be reduced. Converting it to torch.float32. You can silence this warning by converting the value to floating point yourself. If you don't intend to reduce the value (for instance when logging the global step or epoch) then you can use `self.logger.log_metrics({'val_noobj_loss': ...})` instead.\n",
      "c:\\Users\\andre\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:268: You requested to overfit but enabled train dataloader shuffling. We are turning off the train dataloader shuffling for you.\n",
      "c:\\Users\\andre\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\andre\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:293: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/1 [00:00<?, ?it/s] out1:  torch.Size([4, 3072, 6]) - out2:  torch.Size([4, 12288, 6]) - out:  torch.Size([4, 15360, 6])\n",
      "Noobj_loss: 0.9709323048591614\n",
      "Coord_loss: 53.6727180480957\n",
      "Obj_loss: 0.4655582010746002\n",
      "Class_loss: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\andre\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\logger_connector\\result.py:212: You called `self.log('train_noobj_loss', ...)` in your `training_step` but the value needs to be floating to be reduced. Converting it to torch.float32. You can silence this warning by converting the value to floating point yourself. If you don't intend to reduce the value (for instance when logging the global step or epoch) then you can use `self.logger.log_metrics({'train_noobj_loss': ...})` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1/1 [00:12<00:00,  0.08it/s, v_num=3937]Train Loss: 67.2558364868164\n",
      "Epoch 1:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3937]        out1:  torch.Size([4, 3072, 6]) - out2:  torch.Size([4, 12288, 6]) - out:  torch.Size([4, 15360, 6])\n",
      "Noobj_loss: 0.9099503755569458\n",
      "Coord_loss: 94.41686248779297\n",
      "Obj_loss: 0.3892514407634735\n",
      "Class_loss: 0\n",
      "Epoch 1: 100%|██████████| 1/1 [00:01<00:00,  0.65it/s, v_num=3937]Train Loss: 118.16388702392578\n",
      "Epoch 2:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3937]        out1:  torch.Size([4, 3072, 6]) - out2:  torch.Size([4, 12288, 6]) - out:  torch.Size([4, 15360, 6])\n",
      "Noobj_loss: 0.8234810829162598\n",
      "Coord_loss: 53.38959503173828\n",
      "Obj_loss: 0.410013884305954\n",
      "Class_loss: 0\n",
      "Epoch 2: 100%|██████████| 1/1 [00:08<00:00,  0.12it/s, v_num=3937]Train Loss: 66.88066864013672\n",
      "Epoch 3:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3937]        out1:  torch.Size([4, 3072, 6]) - out2:  torch.Size([4, 12288, 6]) - out:  torch.Size([4, 15360, 6])\n",
      "Noobj_loss: 0.7665918469429016\n",
      "Coord_loss: 53.37739181518555\n",
      "Obj_loss: 0.38342103362083435\n",
      "Class_loss: 0\n",
      "Epoch 3: 100%|██████████| 1/1 [00:08<00:00,  0.12it/s, v_num=3937]Train Loss: 66.85592651367188\n",
      "Epoch 4:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3937]        out1:  torch.Size([4, 3072, 6]) - out2:  torch.Size([4, 12288, 6]) - out:  torch.Size([4, 15360, 6])\n",
      "Noobj_loss: 0.7477960586547852\n",
      "Coord_loss: 53.097625732421875\n",
      "Obj_loss: 0.357145220041275\n",
      "Class_loss: 0\n",
      "Epoch 4: 100%|██████████| 1/1 [00:08<00:00,  0.12it/s, v_num=3937]Train Loss: 66.49871063232422\n",
      "Epoch 5:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3937]        out1:  torch.Size([4, 3072, 6]) - out2:  torch.Size([4, 12288, 6]) - out:  torch.Size([4, 15360, 6])\n",
      "Noobj_loss: 0.7399435639381409\n",
      "Coord_loss: 52.2960319519043\n",
      "Obj_loss: 0.33601444959640503\n",
      "Class_loss: 0\n",
      "Epoch 5: 100%|██████████| 1/1 [00:08<00:00,  0.12it/s, v_num=3937]Train Loss: 65.49104309082031\n",
      "Epoch 6:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3937]        out1:  torch.Size([4, 3072, 6]) - out2:  torch.Size([4, 12288, 6]) - out:  torch.Size([4, 15360, 6])\n",
      "Noobj_loss: 0.7368320226669312\n",
      "Coord_loss: 57.4149169921875\n",
      "Obj_loss: 0.31814321875572205\n",
      "Class_loss: 0\n",
      "Epoch 6: 100%|██████████| 1/1 [00:09<00:00,  0.11it/s, v_num=3937]Train Loss: 71.88502502441406\n",
      "Epoch 7:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3937]        out1:  torch.Size([4, 3072, 6]) - out2:  torch.Size([4, 12288, 6]) - out:  torch.Size([4, 15360, 6])\n",
      "Noobj_loss: 0.7367269992828369\n",
      "Coord_loss: 53.13533401489258\n",
      "Obj_loss: 0.34224849939346313\n",
      "Class_loss: 0\n",
      "Epoch 7: 100%|██████████| 1/1 [00:09<00:00,  0.10it/s, v_num=3937]Train Loss: 66.54156494140625\n",
      "Epoch 8:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3937]        out1:  torch.Size([4, 3072, 6]) - out2:  torch.Size([4, 12288, 6]) - out:  torch.Size([4, 15360, 6])\n",
      "Noobj_loss: 0.7366654872894287\n",
      "Coord_loss: 53.3789176940918\n",
      "Obj_loss: 0.3413332402706146\n",
      "Class_loss: 0\n",
      "Epoch 8: 100%|██████████| 1/1 [00:08<00:00,  0.12it/s, v_num=3937]Train Loss: 66.84581756591797\n",
      "Epoch 9:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3937]        out1:  torch.Size([4, 3072, 6]) - out2:  torch.Size([4, 12288, 6]) - out:  torch.Size([4, 15360, 6])\n",
      "Noobj_loss: 0.7360634207725525\n",
      "Coord_loss: 53.42580032348633\n",
      "Obj_loss: 0.33668240904808044\n",
      "Class_loss: 0\n",
      "Epoch 9: 100%|██████████| 1/1 [00:08<00:00,  0.12it/s, v_num=3937]out1:  torch.Size([4, 3072, 6]) - out2:  torch.Size([4, 12288, 6]) - out:  torch.Size([4, 15360, 6])\n",
      "Noobj_loss: 0.7343816757202148\n",
      "Coord_loss: inf\n",
      "Obj_loss: 0.3132617473602295\n",
      "Class_loss: 0\n",
      "Epoch 9: 100%|██████████| 1/1 [00:16<00:00,  0.06it/s, v_num=3937]Train Loss: 66.9032211303711\n",
      "Epoch 9: 100%|██████████| 1/1 [00:16<00:00,  0.06it/s, v_num=3937]\n"
     ]
    }
   ],
   "source": [
    "torch.set_float32_matmul_precision('medium')\n",
    "trainer = pl.Trainer(\n",
    "    accelerator='gpu',\n",
    "    min_epochs=1,\n",
    "    max_epochs=500,\n",
    "    fast_dev_run=False,\n",
    "    overfit_batches=True,\n",
    "    check_val_every_n_epoch=10,\n",
    "    callbacks=[EarlyStopping(monitor='val_loss', mode=\"min\", patience=3)]\n",
    ")\n",
    "# Training\n",
    "#model.device = \"cuda\"\n",
    "#dm.device = \"cuda\"\n",
    "trainer.fit(model, dm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
