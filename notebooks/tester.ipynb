{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pynattas as pnas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture Builder Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code = pnas.functions.architecture_builder.generate_random_architecture_code(6)\n",
    "print(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_architecture = pnas.functions.architecture_builder.parse_architecture_code(code)\n",
    "print(parsed_architecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutated_architecture = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outchannels = [\n",
    "                parsed_architecture[-1]['outchannel1_index'],\n",
    "                parsed_architecture[-1]['outchannel2_index'],\n",
    "                parsed_architecture[-1]['outchannel3_index'],\n",
    "            ]\n",
    "print(outchannels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pnas.functions.architecture_builder.generate_code_from_parsed_architecture(parsed_architecture))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(parsed_architecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind1 = pnas.classes.Individual(3)\n",
    "ind2 = pnas.classes.Individual(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parents = [ind1, ind2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind1.chromosome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "children = pnas.optimizers.ga.single_point_crossover(parents=parents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "children = pnas.optimizers.ga.mutation(children=children,mutation_probability=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_even_numbers(max_value):\n",
    "    if max_value < 4:\n",
    "        raise ValueError(\"The maximum value must be at least 4 to get three different even numbers.\")\n",
    "\n",
    "    # Generate all possible even numbers within the range\n",
    "    even_numbers = [i for i in range(0, max_value + 1) if i % 2 == 0]\n",
    "    \n",
    "    if len(even_numbers) < 3:\n",
    "        raise ValueError(\"Not enough even numbers in the range to choose from.\")\n",
    "\n",
    "    # Ensure the largest even number is included\n",
    "    max_even = even_numbers[-1]\n",
    "    \n",
    "    # Select 2 different even numbers from the list excluding the maximum even number\n",
    "    selected_numbers = random.sample(even_numbers[:-1], 2)\n",
    "    \n",
    "    # Add the largest even number to the list\n",
    "    selected_numbers.append(max_even)\n",
    "    \n",
    "    # Sort the numbers in increasing order\n",
    "    selected_numbers.sort()\n",
    "    \n",
    "    return selected_numbers\n",
    "\n",
    "# Example usage\n",
    "max_value = 10\n",
    "print(generate_even_numbers(max_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_code = pnas.functions.architecture_builder.generate_layer_code()\n",
    "print(layer_code)\n",
    "parsed_layer = pnas.functions.architecture_builder.parse_architecture_code(layer_code)\n",
    "print(parsed_layer)\n",
    "print(pnas.functions.architecture_builder.generate_pooling_layer_code())\n",
    "print(pnas.functions.architecture_builder.generate_head_code())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = pnas.classes.individual.Individual(4)\n",
    "print(ind.architecture)\n",
    "print(ind.chromosome)\n",
    "print(ind.fitness)\n",
    "ind.architecture=ind.chromosome2architecture(ind.chromosome)\n",
    "print(ind.architecture)\n",
    "ind_copy = ind.copy()\n",
    "print(ind_copy.architecture)\n",
    "print(ind_copy.chromosome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_layers_test = [{'layer_type': 'MBConv', 'expansion_factor': 4, 'activation': 'ReLU', 'num_blocks': 1}, {'layer_type': 'AvgPool'}, {'layer_type': 'ConvAct', 'out_channels_coefficient': 2, 'kernel_size': 4, 'stride': 1, 'padding': 0, 'activation': 'ReLU', 'num_blocks': 1}, {'layer_type': 'AvgPool'}, {'layer_type': 'ConvBnAct', 'out_channels_coefficient': 3, 'kernel_size': 5, 'stride': 2, 'padding': 2, 'activation': 'GELU', 'num_blocks': 1}, {'layer_type': 'MaxPool'}, {'layer_type': 'ClassificationHead'}]\n",
    "architecture_code_test = pnas.functions.architecture_builder.generate_code_from_parsed_architecture(parsed_layers_test)\n",
    "print(architecture_code_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing fitness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_layers = pnas.functions.architecture_builder.parse_architecture_code(ind.architecture)\n",
    "fitness = pnas.functions.fitness.compute_fitness_value(parsed_layers=parsed_layers)\n",
    "print(fitness)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Genetic Algorithm Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('config.ini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_layers = config.getint('NAS', 'max_layers')\n",
    "#max_iterations = int(config['GA']['max_iterations'])\n",
    "max_iterations = 3\n",
    "#population_size = int(config['GA']['population_size'])\n",
    "population_size = 4\n",
    "log_path = str(config['GA']['logs_dir_GA'])\n",
    "mating_pool_cutoff = float(config['GA']['mating_pool_cutoff'])\n",
    "mutation_probability = float(config['GA']['mutation_probability'])\n",
    "nas_result = pnas.optimizers.ga.ga_optimizer(\n",
    "    max_layers=max_layers,\n",
    "    max_iter=max_iterations,\n",
    "    n_individuals=population_size,\n",
    "    mating_pool_cutoff=mating_pool_cutoff,\n",
    "    mutation_probability=mutation_probability,\n",
    "    logs_directory=log_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HT TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "import pynattas as pnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('config.ini')\n",
    "\n",
    "# Use NAS result if available, otherwise load from config\n",
    "architecture_code = config['NAS']['architecture_code']\n",
    "layers = pnas.functions.architecture_builder.parse_architecture_code(architecture_code)\n",
    "print('HT Layers:', layers)\n",
    "best_position = []  # initialize final best position for final run\n",
    "\n",
    "print(\"\\n*** Hyperparameter Tuning ***\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define HT search space\n",
    "search_space = pnas.functions.utils.generate_layers_search_space(layers)\n",
    "search_space['log_learning_rate'] = (\n",
    "    float(config['Search Space']['log_lr_min']),\n",
    "    float(config['Search Space']['log_lr_max']),\n",
    ")\n",
    "search_space['batch_size'] = (\n",
    "    float(config['Search Space']['bs_min']),\n",
    "    float(config['Search Space']['bs_max']),\n",
    ")\n",
    "\n",
    "print('Search space:', search_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycocotools.coco import COCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stuff about data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.coco128.dataset import COCODetectionDataset, COCODetectionDataModule\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir='data/coco128'\n",
    "full_dataset = COCODetectionDataset(\n",
    "        img_dir=root_dir + '/images/train2017',\n",
    "        ann_file=root_dir + '/labels/train2017/annotations.json',\n",
    "        transform=None,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset[2][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define category names\n",
    "category_names = {\n",
    "    0: \"person\", 1: \"bicycle\", 2: \"car\", 3: \"motorcycle\", 4: \"airplane\",\n",
    "    5: \"bus\", 6: \"train\", 7: \"truck\", 8: \"boat\", 9: \"traffic light\",\n",
    "    10: \"fire hydrant\", 11: \"stop sign\", 12: \"parking meter\", 13: \"bench\", 14: \"bird\",\n",
    "    15: \"cat\", 16: \"dog\", 17: \"horse\", 18: \"sheep\", 19: \"cow\",\n",
    "    20: \"elephant\", 21: \"bear\", 22: \"zebra\", 23: \"giraffe\", 24: \"backpack\",\n",
    "    25: \"umbrella\", 26: \"handbag\", 27: \"tie\", 28: \"suitcase\", 29: \"frisbee\",\n",
    "    30: \"skis\", 31: \"snowboard\", 32: \"sports ball\", 33: \"kite\", 34: \"baseball bat\",\n",
    "    35: \"baseball glove\", 36: \"skateboard\", 37: \"surfboard\", 38: \"tennis racket\", 39: \"bottle\",\n",
    "    40: \"wine glass\", 41: \"cup\", 42: \"fork\", 43: \"knife\", 44: \"spoon\",\n",
    "    45: \"bowl\", 46: \"banana\", 47: \"apple\", 48: \"sandwich\", 49: \"orange\",\n",
    "    50: \"broccoli\", 51: \"carrot\", 52: \"hot dog\", 53: \"pizza\", 54: \"donut\",\n",
    "    55: \"cake\", 56: \"chair\", 57: \"couch\", 58: \"potted plant\", 59: \"bed\",\n",
    "    60: \"dining table\", 61: \"toilet\", 62: \"tv\", 63: \"laptop\", 64: \"mouse\",\n",
    "    65: \"remote\", 66: \"keyboard\", 67: \"cell phone\", 68: \"microwave\", 69: \"oven\",\n",
    "    70: \"toaster\", 71: \"sink\", 72: \"refrigerator\", 73: \"book\", 74: \"clock\",\n",
    "    75: \"vase\", 76: \"scissors\", 77: \"teddy bear\", 78: \"hair drier\", 79: \"toothbrush\"\n",
    "}\n",
    "\n",
    "\n",
    "def draw_bbox_cc_oo(pil_image, bboxes, labels):\n",
    "    # Create a drawing context\n",
    "    draw = ImageDraw.Draw(pil_image)\n",
    "    \n",
    "    for idx, bbox in enumerate(bboxes):\n",
    "        # Extract the center coordinates and offsets from the tensor\n",
    "        cx, cy, w, h = bbox.tolist()\n",
    "        \n",
    "        # Correct normalization\n",
    "        width, height = pil_image.size; print(f\"Size {width} by {height}\")\n",
    "        cx *= width\n",
    "        cy *= height\n",
    "        w *= width\n",
    "        h *= height\n",
    "\n",
    "        print(f\"c:({cx},{cy}), offset:({w},{h})\")\n",
    "\n",
    "        # Calculate the top-left and bottom-right coordinates\n",
    "        x1 = cx - w/2\n",
    "        y1 = cy - h/2\n",
    "        x2 = cx + w/2\n",
    "        y2 = cy + h/2\n",
    "        \n",
    "        # Draw the bounding box in red\n",
    "        draw.rectangle([x1, y1, x2, y2], outline=\"red\", width=2)\n",
    "\n",
    "        # Write class name\n",
    "        draw.text(\n",
    "            xy=[x1, y1], \n",
    "            text=f\"{category_names[int(labels[idx])]}\", \n",
    "            font=ImageFont.truetype(\"arial.ttf\", 15),\n",
    "            fill=(255, 0, 0),\n",
    "            )\n",
    "    \n",
    "    # Display the image with the bounding box using matplotlib\n",
    "    plt.imshow(pil_image)\n",
    "    plt.axis('off')  # Hide the axis\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_bbox_cc_oo(full_dataset[2][0], full_dataset[2][1]['boxes'], full_dataset[2][1]['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING EVERYTHING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pynattas as pnas\n",
    "from torchvision import transforms\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "import torch\n",
    "from datasets.coco128.dataset import COCODetectionDataset, COCODetectionDataModule\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir='data/coco128'\n",
    "input_size = 416\n",
    "custom_transform = transforms.Compose([\n",
    "    transforms.Resize((input_size, input_size)),  # Resize images to the size expected by your model\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "full_dataset = COCODetectionDataset(\n",
    "        img_dir=root_dir + '/images/train2017',\n",
    "        ann_file=root_dir + '/labels/train2017/annotations.json',\n",
    "        transform=custom_transform,\n",
    "    )\n",
    "\n",
    "'''\n",
    "batch = full_dataset[0:3]\n",
    "for i, b in enumerate(batch):\n",
    "    print(f\"{i} : {b}\")\n",
    "\n",
    "batch = full_dataset\n",
    "images = []\n",
    "targets = []\n",
    "#targets = {'boxes': [], 'labels': [], 'image_id': []}\n",
    "\n",
    "for b in batch:\n",
    "    images.append(b[0])\n",
    "\n",
    "    targets_tmp = {'boxes': [], 'labels': [], 'image_id': []}\n",
    "    targets_tmp['boxes'].append(torch.tensor(np.array(b[1]['boxes'])))\n",
    "    targets_tmp['labels'].append(torch.tensor(np.array(b[1]['labels'])))\n",
    "    targets_tmp['image_id'].append(torch.tensor(np.array(b[1]['image_id'])))\n",
    "    targets.append(targets_tmp)\n",
    "\n",
    "# Stack images\n",
    "images = torch.stack([torch.tensor(np.array(img)) for img in images], dim=0)\n",
    "print(\"Image: \", images[2])\n",
    "print(\"Bboxes: \", targets[2])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir='data/coco128'\n",
    "input_size = 416\n",
    "custom_transform = transforms.Compose([\n",
    "    transforms.Resize((input_size, input_size)),  # Resize images to the size expected by your model\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "dm = COCODetectionDataModule(\n",
    "        img_dir=root_dir + '/images/train2017',\n",
    "        ann_file=root_dir + '/labels/train2017/annotations.json',\n",
    "        batch_size=4,\n",
    "        num_workers=0,\n",
    "        transform=custom_transform,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code = pnas.functions.architecture_builder.generate_random_architecture_code(5)\n",
    "print(code)\n",
    "parsed_layers = pnas.functions.architecture_builder.parse_architecture_code(code)\n",
    "print(parsed_layers)\n",
    "\n",
    "model = pnas.classes.GenericOD_YOLOv3(\n",
    "    parsed_layers=parsed_layers,\n",
    "    input_channels=3,\n",
    "    input_height=416,\n",
    "    input_width=416,\n",
    "    num_classes=80,\n",
    "    learning_rate=1e-2,\n",
    ")\n",
    "if model.model.is_too_deep:\n",
    "    print(\"This architecture is too deep and the tensor lost its dimensionality\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "# Generate a random tensor with the appropriate shape\n",
    "input_tensor = torch.randn(1, 3, 416, 416)  # Batch size of 1, 3 channels, 416x416 height and width\n",
    "\n",
    "# Pass the tensor through the model\n",
    "output = model(input_tensor)\n",
    "\n",
    "# Optional: Print the output shape to verify\n",
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the first 1/25 is for scale l, 4/25 is for scale m, 20/25 is for scale s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision('medium')\n",
    "trainer = pl.Trainer(\n",
    "    accelerator='gpu',\n",
    "    min_epochs=1,\n",
    "    max_epochs=500,\n",
    "    fast_dev_run=False,\n",
    "    overfit_batches=True,\n",
    "    check_val_every_n_epoch=10,\n",
    "    callbacks=[EarlyStopping(monitor='val_loss', mode=\"min\", patience=3)]\n",
    ")\n",
    "# Training\n",
    "#model.device = \"cuda\"\n",
    "#dm.device = \"cuda\"\n",
    "trainer.fit(model, dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INFERENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "import pynattas as pnas\n",
    "from datasets.coco128.dataset import COCODetectionDataset, COCODetectionDataModule\n",
    "from datasets.SPECTRE_prova.dataset import SPECTRE_COCO_Dataset, SPECTRE_COCO_DataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, image, top_k=100, iou_threshold=0.5):\n",
    "    # Step 1: Preprocess the image\n",
    "    original_img, img = preprocess(image)  # Preprocess returns a tensor and the original PIL image\n",
    "\n",
    "    # Step 2: Run the model to get predictions\n",
    "    with torch.no_grad():\n",
    "        preds = model(img.unsqueeze(0))  # Add batch dimension\n",
    "\n",
    "    # Step 3: Select top_k predictions based on objectness scores\n",
    "    objectness_scores = preds[0,..., 4]\n",
    "    top_k_indices = objectness_scores.argsort(descending=True)[:top_k]\n",
    "    preds = preds[0, top_k_indices, :]\n",
    "    #keep = objectness_scores > obj_threshold\n",
    "    #preds = preds[keep]\n",
    "\n",
    "    # Step 4: Apply Non-Max Suppression (NMS)\n",
    "    preds = apply_nms(preds, iou_threshold)\n",
    "\n",
    "    # Step 5: Scale predictions to image size\n",
    "    img_size = original_img.size  # Width, Height\n",
    "    preds[:, :4] = scale_boxes(preds[:, :4], img_size)\n",
    "\n",
    "    # Step 6: Draw bounding boxes\n",
    "    draw_boxes(original_img, preds)\n",
    "\n",
    "def apply_nms(preds, iou_threshold):\n",
    "    bboxes = preds[:, :4]\n",
    "    scores = preds[:, 4]\n",
    "    classes = preds[:, 5:].argmax(dim=-1)\n",
    "\n",
    "    keep_boxes = []\n",
    "    unique_classes = classes.unique()\n",
    "    for cls in unique_classes:\n",
    "        cls_mask = classes == cls\n",
    "        cls_bboxes = bboxes[cls_mask]\n",
    "        cls_scores = scores[cls_mask]\n",
    "\n",
    "        keep = nms(cls_bboxes, cls_scores, iou_threshold)\n",
    "        keep_boxes.append(preds[keep])\n",
    "\n",
    "    return torch.cat(keep_boxes, dim=0)\n",
    "\n",
    "def nms(bboxes, scores, iou_threshold):\n",
    "    idxs = scores.argsort(descending=True)\n",
    "    keep = []\n",
    "\n",
    "    while idxs.numel() > 0:\n",
    "        i = idxs[0]\n",
    "        keep.append(i)\n",
    "\n",
    "        if idxs.numel() == 1:\n",
    "            break\n",
    "\n",
    "        ious = bboxes_iou(bboxes[i].unsqueeze(0), bboxes[idxs[1:]])\n",
    "        idxs = idxs[1:][ious <= iou_threshold]\n",
    "\n",
    "    return torch.tensor(keep, dtype=torch.long, device=bboxes.device)\n",
    "\n",
    "def bboxes_iou(box1, box2):\n",
    "    inter_rect_x1 = torch.max(box1[:, 0], box2[:, 0])\n",
    "    inter_rect_y1 = torch.max(box1[:, 1], box2[:, 1])\n",
    "    inter_rect_x2 = torch.min(box1[:, 2], box2[:, 2])\n",
    "    inter_rect_y2 = torch.min(box1[:, 3], box2[:, 3])\n",
    "\n",
    "    inter_area = torch.clamp(inter_rect_x2 - inter_rect_x1 + 1, min=0) * torch.clamp(inter_rect_y2 - inter_rect_y1 + 1, min=0)\n",
    "    b1_area = (box1[:, 2] - box1[:, 0] + 1) * (box1[:, 3] - box1[:, 1] + 1)\n",
    "    b2_area = (box2[:, 2] - box2[:, 0] + 1) * (box2[:, 3] - box2[:, 1] + 1)\n",
    "\n",
    "    iou = inter_area / (b1_area + b2_area - inter_area + 1e-9)\n",
    "    return iou\n",
    "\n",
    "def scale_boxes(boxes, img_size):\n",
    "    width, height = img_size\n",
    "    boxes[:, 0] *= width\n",
    "    boxes[:, 1] *= height\n",
    "    boxes[:, 2] *= width\n",
    "    boxes[:, 3] *= height\n",
    "    return boxes\n",
    "\n",
    "def draw_boxes(image, preds):\n",
    "    # Define category names\n",
    "    category_names = {\n",
    "        0: \"person\", 1: \"bicycle\", 2: \"car\", 3: \"motorcycle\", 4: \"airplane\",\n",
    "        5: \"bus\", 6: \"train\", 7: \"truck\", 8: \"boat\", 9: \"traffic light\",\n",
    "        10: \"fire hydrant\", 11: \"stop sign\", 12: \"parking meter\", 13: \"bench\", 14: \"bird\",\n",
    "        15: \"cat\", 16: \"dog\", 17: \"horse\", 18: \"sheep\", 19: \"cow\",\n",
    "        20: \"elephant\", 21: \"bear\", 22: \"zebra\", 23: \"giraffe\", 24: \"backpack\",\n",
    "        25: \"umbrella\", 26: \"handbag\", 27: \"tie\", 28: \"suitcase\", 29: \"frisbee\",\n",
    "        30: \"skis\", 31: \"snowboard\", 32: \"sports ball\", 33: \"kite\", 34: \"baseball bat\",\n",
    "        35: \"baseball glove\", 36: \"skateboard\", 37: \"surfboard\", 38: \"tennis racket\", 39: \"bottle\",\n",
    "        40: \"wine glass\", 41: \"cup\", 42: \"fork\", 43: \"knife\", 44: \"spoon\",\n",
    "        45: \"bowl\", 46: \"banana\", 47: \"apple\", 48: \"sandwich\", 49: \"orange\",\n",
    "        50: \"broccoli\", 51: \"carrot\", 52: \"hot dog\", 53: \"pizza\", 54: \"donut\",\n",
    "        55: \"cake\", 56: \"chair\", 57: \"couch\", 58: \"potted plant\", 59: \"bed\",\n",
    "        60: \"dining table\", 61: \"toilet\", 62: \"tv\", 63: \"laptop\", 64: \"mouse\",\n",
    "        65: \"remote\", 66: \"keyboard\", 67: \"cell phone\", 68: \"microwave\", 69: \"oven\",\n",
    "        70: \"toaster\", 71: \"sink\", 72: \"refrigerator\", 73: \"book\", 74: \"clock\",\n",
    "        75: \"vase\", 76: \"scissors\", 77: \"teddy bear\", 78: \"hair drier\", 79: \"toothbrush\"\n",
    "    }\n",
    "\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    #font = ImageFont.truetype('arial.ttf', 15)\n",
    "\n",
    "    for pred in preds:\n",
    "        xc, yc, w, h = pred[:4]\n",
    "        x1 = xc - w/2\n",
    "        y1 = yc - w/2\n",
    "        x2 = xc + w/2\n",
    "        y2 = yc + w/2\n",
    "        class_id = pred[5:].argmax().item()\n",
    "        label = category_names[class_id]\n",
    "        draw.rectangle([x1, y1, x2, y2], outline=\"red\", width=2)\n",
    "        draw.text((x1, y1), label, fill=\"red\") #, font=font)\n",
    "\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Helper function to preprocess the image\n",
    "def preprocess(image_path):\n",
    "    img = Image.open(image_path).convert('RGB')\n",
    "    width, height = img.size\n",
    "\n",
    "    custom_transform = transforms.Compose([\n",
    "        transforms.Resize((416, 416)),  # Resize images to the size expected by your model\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    img_tensor = custom_transform(img)\n",
    "        \n",
    "    return img, img_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "architecture_code = \"Lne5arn1EPMELeo2k4s2p2aln1EPMELme3aln1EPIELRr3agn1EPMELoaln1EPIEHYu2v4w8EE\"\n",
    "model = pnas.classes.GenericOD_YOLOv3(\n",
    "    parsed_layers=pnas.functions.architecture_builder.parse_architecture_code(architecture_code),\n",
    "    input_channels=3,\n",
    "    input_height=416,\n",
    "    input_width=416,\n",
    "    num_classes=80,\n",
    "    learning_rate=1e-3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = config.getint(section='DetectionHeadYOLOv3', option='image_size')\n",
    "transform = transforms.Compose([\n",
    "     transforms.Resize((input_size, input_size)),  # Resize images to the size expected by your model\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "root_dir = \"data/coco128\"\n",
    "dm = COCODetectionDataModule(\n",
    "     img_dir=root_dir + '/images/train2017',\n",
    "     ann_file=root_dir + '/labels/train2017/annotations.json',\n",
    "     batch_size=1,\n",
    "     num_workers=0,\n",
    "     transform=transform,\n",
    ")\n",
    "num_classes = 80\n",
    "in_channels = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"./logs/tb_logs/checkpoints/OptimizedModel_2024-06-27_18-18-46/version_0/checkpoints/epoch=169-step=10710.ckpt\")\n",
    "model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"data/coco128/images/train2017/000000000034.jpg\"\n",
    "inference(model, image_path, 100, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensing the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the EPSILON for numerical stability\n",
    "EPSILON = 1e-9\n",
    "\n",
    "# Example input tensors\n",
    "bboxes1 = torch.tensor([\n",
    "    [\n",
    "        [0.1, 0.2, 0.3, 0.4],  # Box 1 in sample 1\n",
    "        [0.5, 0.6, 0.1, 0.2],  # Box 2 in sample 1\n",
    "        [0.7, 0.8, 0.2, 0.1]   # Box 3 in sample 1\n",
    "    ],\n",
    "    [\n",
    "        [0.2, 0.3, 0.4, 0.5],  # Box 1 in sample 2\n",
    "        [0.6, 0.7, 0.3, 0.2],   # Box 2 in sample 2\n",
    "        [0, 0, 0, 0]   # Box 2 in sample 2\n",
    "    ]\n",
    "], device='cuda:0')\n",
    "\n",
    "bboxes2 = torch.tensor([\n",
    "    [\n",
    "        [0.2, 0.3, 0.1, 0.1],  # Box 1 in sample 1\n",
    "        [-0.4, 0.5, 0.2, 0.2],   # Box 2 in sample 1\n",
    "        [0.2, -0.3, -0.1, 0.1],  # Box 1 in sample 1\n",
    "        [-0.4, 0.5, 0.2, 0.2],   # Box 2 in sample 1\n",
    "        [0.4, 0.5, 0.2, 0.2],   # Box 2 in sample 1\n",
    "    ],\n",
    "    [\n",
    "        [0.5, 0.6, 0.3, 0.4],  # Box 1 in sample 2\n",
    "        [0.7, 0.8, 0.2, -0.2],  # Box 2 in sample 2\n",
    "        [0.1, 0.2, 0.1, 0.1],   # Box 3 in sample 2\n",
    "        [-0.4, 0.5, 0.2, 0.2],   # Box 2 in sample 1\n",
    "        [0.4, 0.5, -0.2, 0.2],   # Box 2 in sample 1\n",
    "    ]\n",
    "], device='cuda:0')\n",
    "\n",
    "# Print shapes to verify\n",
    "print(\"Shape of bboxes1:\", bboxes1.shape)\n",
    "print(\"Shape of bboxes2:\", bboxes2.shape)\n",
    "\n",
    "# Define the iou_batch function\n",
    "def iou_batch(bboxes1: torch.Tensor, bboxes2: torch.Tensor, center=False, zero_center=False):\n",
    "    x1 = bboxes1[..., 0]\n",
    "    y1 = bboxes1[..., 1]\n",
    "    w1 = bboxes1[..., 2]\n",
    "    h1 = bboxes1[..., 3]\n",
    "\n",
    "    x2 = bboxes2[..., 0]\n",
    "    y2 = bboxes2[..., 1]\n",
    "    w2 = bboxes2[..., 2]\n",
    "    h2 = bboxes2[..., 3]\n",
    "\n",
    "    area1 = w1 * h1\n",
    "    area2 = w2 * h2\n",
    "\n",
    "    if zero_center:\n",
    "        w1.unsqueeze_(2)\n",
    "        w2.unsqueeze_(1)\n",
    "        h1.unsqueeze_(2)\n",
    "        h2.unsqueeze_(1)\n",
    "        w_intersect = torch.min(w1, w2).clamp(min=0)\n",
    "        h_intersect = torch.min(h1, h2).clamp(min=0)\n",
    "    else:\n",
    "        if center:\n",
    "            x1 = x1 - w1 / 2\n",
    "            y1 = y1 - h1 / 2\n",
    "            x2 = x2 - w2 / 2\n",
    "            y2 = y2 - h2 / 2\n",
    "        right1 = (x1 + w1).unsqueeze(2)\n",
    "        right2 = (x2 + w2).unsqueeze(1)\n",
    "        top1 = (y1 + h1).unsqueeze(2)\n",
    "        top2 = (y2 + h2).unsqueeze(1)\n",
    "        left1 = x1.unsqueeze(2)\n",
    "        left2 = x2.unsqueeze(1)\n",
    "        bottom1 = y1.unsqueeze(2)\n",
    "        bottom2 = y2.unsqueeze(1)\n",
    "        w_intersect = (torch.min(right1, right2) - torch.max(left1, left2)).clamp(min=0)\n",
    "        h_intersect = (torch.min(top1, top2) - torch.max(bottom1, bottom2)).clamp(min=0)\n",
    "    area_intersect = h_intersect * w_intersect\n",
    "\n",
    "    iou_ = area_intersect / (area1.unsqueeze(2) + area2.unsqueeze(1) - area_intersect + EPSILON)\n",
    "\n",
    "    return iou_\n",
    "\n",
    "# Calculate IoUs\n",
    "ious = iou_batch(bboxes1, bboxes2, center=True)\n",
    "\n",
    "# Print the IoUs\n",
    "print(\"IoUs:\", ious)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt = torch.tensor([[[7.3943e-01, 4.8450e-01, 2.9297e-02, 1.3125e-02, 1.0000e+00,\n",
    "          2.9000e+01],\n",
    "         [6.5723e-01, 5.7649e-01, 2.2723e-01, 3.8394e-01, 1.0000e+00,\n",
    "          0.0000e+00],\n",
    "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
    "          0.0000e+00],\n",
    "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
    "          0.0000e+00],\n",
    "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
    "          0.0000e+00],\n",
    "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
    "          0.0000e+00],\n",
    "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
    "          0.0000e+00]],\n",
    "\n",
    "        [[8.5887e-01, 1.0215e-01, 2.2419e-01, 2.0429e-01, 1.0000e+00,\n",
    "          5.6000e+01],\n",
    "         [4.9972e-01, 5.2134e-01, 9.8597e-01, 8.5844e-01, 1.0000e+00,\n",
    "          5.3000e+01],\n",
    "         [2.8711e-02, 6.8856e-01, 5.7422e-02, 1.4975e-01, 1.0000e+00,\n",
    "          4.1000e+01],\n",
    "         [8.6786e-01, 8.2208e-01, 2.3572e-01, 3.2987e-01, 1.0000e+00,\n",
    "          4.1000e+01],\n",
    "         [9.6503e-01, 8.9704e-01, 6.9937e-02, 1.0854e-01, 1.0000e+00,\n",
    "          4.2000e+01],\n",
    "         [3.9546e-01, 8.6364e-01, 7.8702e-01, 2.7273e-01, 1.0000e+00,\n",
    "          6.0000e+01],\n",
    "         [1.8214e-01, 2.1389e-01, 3.5259e-01, 4.2777e-01, 1.0000e+00,\n",
    "          0.0000e+00]],\n",
    "\n",
    "        [[6.8510e-01, 3.2040e-01, 1.3627e-01, 1.3920e-01, 1.0000e+00,\n",
    "          4.0000e+00],\n",
    "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
    "          0.0000e+00],\n",
    "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
    "          0.0000e+00],\n",
    "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
    "          0.0000e+00],\n",
    "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
    "          0.0000e+00],\n",
    "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
    "          0.0000e+00],\n",
    "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
    "          0.0000e+00]],\n",
    "\n",
    "        [[2.4388e-01, 3.1009e-01, 1.9252e-01, 4.8711e-01, 1.0000e+00,\n",
    "          3.9000e+01],\n",
    "         [7.0117e-02, 2.9029e-01, 1.3916e-01, 1.3492e-01, 1.0000e+00,\n",
    "          5.6000e+01],\n",
    "         [6.7366e-01, 3.3052e-01, 6.5269e-01, 4.5161e-01, 1.0000e+00,\n",
    "          5.6000e+01],\n",
    "         [4.9610e-01, 6.5764e-01, 9.9220e-01, 6.5647e-01, 1.0000e+00,\n",
    "          6.0000e+01],\n",
    "         [4.8806e-01, 4.1148e-01, 2.0621e-01, 3.3472e-01, 1.0000e+00,\n",
    "          4.0000e+01],\n",
    "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
    "          0.0000e+00],\n",
    "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
    "          0.0000e+00]]], device='cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bboxes_tgt = tgt[..., :4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bboxes_tgt[..., 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVEN MORE TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "Tensor = torch.Tensor\n",
    "ANCHORS = [ # obtained by normalizing over 416 the classic anchors [(10, 13), (16, 30), (33, 23), (30, 61), (62, 45), (59, 119), (116, 90), (156, 198), (373, 326)]\n",
    "    (0.02403846153846154, 0.03125),\n",
    "    (0.038461538461538464, 0.07211538461538461),\n",
    "    (0.07932692307692307, 0.055288461538461536),\n",
    "    (0.07211538461538461, 0.1466346153846154),\n",
    "    (0.14903846153846154, 0.10817307692307693),\n",
    "    (0.14182692307692307, 0.2860576923076923),\n",
    "    (0.27884615384615385, 0.21634615384615385),\n",
    "    (0.375, 0.47596153846153844),\n",
    "    (0.8966346153846154, 0.7836538461538461)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function\n",
    "def yolo_loss_fn(preds: Tensor, tgt: Tensor, tgt_len: Tensor, img_size: int, average=True):\n",
    "    \"\"\"Calculate the loss function given the predictions, the targets, the length of each target and the image size.\"\"\"\n",
    "    #print(f\"Preds shape: {preds.shape}\")\n",
    "    #print(f\"Tgt shape: {tgt.shape}\")\n",
    "    #print(f\"Tgt_len: {tgt_len}\")\n",
    "    #print(f\"Img_size: {img_size}\")\n",
    "    if torch.isnan(preds).any():\n",
    "        print(\"NaNs found in preds before processing\")\n",
    "\n",
    "    # generate the no-objectness mask. mask_noobj has size of [B, N_PRED]\n",
    "    mask_noobj = noobj_mask_fn(preds, tgt)\n",
    "    print(\"The number of non_zeros in mask_noobj is:\", torch.count_nonzero(mask_noobj))\n",
    "    print(f\"Mask_noobj shape: {mask_noobj.shape}\")\n",
    "    print(f\"mask_noobj - {mask_noobj}\")\n",
    "\n",
    "    tgt_t_1d, idx_pred_obj = pre_process_targets(tgt, tgt_len, img_size)\n",
    "    #print(f\"Tgt_t_1d shape: {tgt_t_1d.shape}\")\n",
    "    print(\"tgt_t_1d - \", tgt_t_1d)\n",
    "    print(\"idx_pred_obj - \", idx_pred_obj)\n",
    "    #print(f\"Idx_pred_obj shape: {idx_pred_obj.shape}\")\n",
    "\n",
    "    mask_noobj = noobj_mask_filter(mask_noobj, idx_pred_obj)\n",
    "    print(\"The number of non_zeros in mask_noobj after filter is:\", torch.count_nonzero(mask_noobj))\n",
    "    print(f\"Mask_noobj after filter shape: {mask_noobj.shape}\")\n",
    "\n",
    "    # calculate the no-objectness loss\n",
    "    pred_conf_logit = preds[..., 4]\n",
    "    #print(f\"pred_conf_logit before: {pred_conf_logit}\")\n",
    "    tgt_zero = torch.zeros(pred_conf_logit.size(), device=pred_conf_logit.device)\n",
    "    pred_conf_logit = pred_conf_logit - (1 - mask_noobj) * 1e6\n",
    "    noobj_loss = F.binary_cross_entropy_with_logits(pred_conf_logit, tgt_zero, reduction='sum')\n",
    "    print(f\"Noobj_loss: {noobj_loss}\")\n",
    "\n",
    "    # select the predictions corresponding to the targets\n",
    "    n_batch, n_pred, _ = preds.size()\n",
    "    preds_1d = preds.view(n_batch * n_pred, -1)\n",
    "    preds_obj = preds_1d.index_select(0, idx_pred_obj)\n",
    "    print(f\"Preds_obj shape: {preds_obj.shape}\")\n",
    "\n",
    "    # calculate the coordinate loss\n",
    "    coord_loss = F.mse_loss(preds_obj[..., :4], tgt_t_1d[..., :4], reduction='sum')\n",
    "    print(f\"Coord_loss: {coord_loss}\")\n",
    "\n",
    "    # calculate the objectness loss\n",
    "    pred_conf_obj_logit = preds_obj[..., 4]\n",
    "    tgt_one = torch.ones(pred_conf_obj_logit.size(), device=pred_conf_obj_logit.device)\n",
    "    obj_loss = F.binary_cross_entropy_with_logits(pred_conf_obj_logit, tgt_one, reduction='sum')\n",
    "    print(f\"Obj_loss: {obj_loss}\")\n",
    "\n",
    "    # Convert class indices to one-hot encoding\n",
    "    num_classes = preds.shape[-1] - 5\n",
    "    tgt_t_1d_classes = F.one_hot(tgt_t_1d[..., 5].long(), num_classes).float()\n",
    "    print(f\"Tgt_t_1d_classes shape: {tgt_t_1d_classes.shape}\")\n",
    "\n",
    "    # calculate the classification loss\n",
    "    class_loss = F.binary_cross_entropy_with_logits(preds_obj[..., 5:], tgt_t_1d_classes, reduction='sum')\n",
    "    print(f\"Class_loss: {class_loss}\")\n",
    "\n",
    "    # total loss\n",
    "    noobj_coeff = 0.2\n",
    "    coord_coeff = 5\n",
    "    total_loss = noobj_loss * noobj_coeff + obj_loss + class_loss + coord_loss * coord_coeff\n",
    "\n",
    "    if average:\n",
    "        total_loss = total_loss / n_batch\n",
    "    \n",
    "    print(f\"Total_loss: {total_loss}\")\n",
    "    return total_loss, coord_loss, obj_loss, noobj_loss, class_loss\n",
    "\n",
    "def noobj_mask_fn(pred: Tensor, target: Tensor):\n",
    "    ignore_threshold = 0.5\n",
    "    num_batch, num_pred, num_attrib = pred.size()\n",
    "    assert num_batch == target.size(0)\n",
    "    \n",
    "    # Calculate IoU between decoded predictions and targets\n",
    "    ious = iou_batch(pred[..., :4], target[..., :4], center=True) #in cxcywh format\n",
    "    print(\"IoUs shape in noobj_mask_fn:\", ious.shape)\n",
    "    print(f\"IoU min: {ious.min().item()}, max: {ious.max().item()}, mean: {ious.mean().item()}, std: {ious.std().item()}\")\n",
    "\n",
    "    # for each pred bbox, find the target box which overlaps with it (without zero centered) most, and the iou value.\n",
    "    max_ious, max_ious_idx = torch.max(ious, dim=2)\n",
    "    print(\"Max_IoUs shape in noobj_mask_fn:\", max_ious.shape)\n",
    "    noobj_indicator = torch.where((max_ious - ignore_threshold) > 0, torch.zeros_like(max_ious), torch.ones_like(max_ious))\n",
    "    \n",
    "    return noobj_indicator\n",
    "\n",
    "def noobj_mask_filter(mask_noobj: Tensor, idx_obj_1d: Tensor):\n",
    "    n_batch, n_pred = mask_noobj.size()\n",
    "    mask_noobj = mask_noobj.view(-1)\n",
    "    filter_ = torch.zeros(mask_noobj.size(), device=mask_noobj.device)\n",
    "    mask_noobj.scatter_(0, idx_obj_1d, filter_)\n",
    "    mask_noobj = mask_noobj.view(n_batch, -1)\n",
    "    return mask_noobj\n",
    "\n",
    "def pre_process_targets(tgt: Tensor, tgt_len, img_size):\n",
    "    epsilon = 1e-9\n",
    "\n",
    "    # Initializes the anchor boxes and creates a tensor of shape [1, n_anchor, 4] where n_anchor is the number of anchors. \n",
    "    # Each anchor box has zero-centered coordinates (cx, cy, w, h).\n",
    "    wh_anchor = torch.tensor(ANCHORS).to(tgt.device).float()\n",
    "    n_anchor = wh_anchor.size(0)\n",
    "    xy_anchor = torch.zeros((n_anchor, 2), device=tgt.device)\n",
    "    bbox_anchor = torch.cat((xy_anchor, wh_anchor), dim=1)\n",
    "    bbox_anchor.unsqueeze_(0)\n",
    "\n",
    "    #print(\"bbox_anchor:\", bbox_anchor)\n",
    "    #bbox_anchor /= img_size + epsilon\n",
    "    #print(\"bbox_anchor/img_size:\", bbox_anchor)\n",
    "\n",
    "    # Computes the Intersection Over Union (IOU) between each anchor and target box, then finds the anchor with the maximum \n",
    "    # IOU for each target.\n",
    "    iou_anchor_tgt = iou_batch(bbox_anchor, tgt[..., :4], zero_center=True)\n",
    "    #print(\"iou_anchor_tgt:\", iou_anchor_tgt)\n",
    "    _, idx_anchor = torch.max(iou_anchor_tgt, dim=1)\n",
    "    #print(\"idx_anchor:\", idx_anchor)\n",
    "\n",
    "    # Calculates the grid cell coordinates (grid_x, grid_y) where each target falls, based on the stride of the respective scale (8, 16, or 32).\n",
    "    # Determines the corresponding prediction index for each target, taking into account the scale and the position within the grid.\n",
    "    strides_selection = [8, 16, 32]\n",
    "    scale = idx_anchor // 3\n",
    "    #print(\"scale:\", scale)\n",
    "    idx_anchor_by_scale = idx_anchor - scale * 3\n",
    "    #print(\"idx_anchor_by_scale:\", idx_anchor_by_scale)\n",
    "    stride = 8 * 2 ** scale\n",
    "    #print(\"stride:\", stride)\n",
    "    #print(\"tgt[..., 0]:\", tgt[..., 0], \"tgt[..., 1]:\", tgt[..., 1])\n",
    "    grid_x = ((tgt[..., 0]*img_size) // stride.float()).long()\n",
    "    grid_y = ((tgt[..., 1]*img_size) // stride.float()).long()\n",
    "    #print(\"grid_x:\", grid_x, \"grid_y:\", grid_y)\n",
    "    n_grid = img_size // stride\n",
    "    #print(\"n_grid:\", n_grid)\n",
    "    large_scale_mask = (scale <= 1).long()\n",
    "    med_scale_mask = (scale <= 0).long()\n",
    "    idx_obj = \\\n",
    "        large_scale_mask * (img_size // strides_selection[2]) ** 2 * 3 + \\\n",
    "        med_scale_mask * (img_size // strides_selection[1]) ** 2 * 3 + \\\n",
    "        n_grid ** 2 * idx_anchor_by_scale + n_grid * grid_y + grid_x\n",
    "    #print(\"idx_obj:\", idx_obj)\n",
    "\n",
    "    # Calculate Local Coordinates (tx, ty, tw, th):\n",
    "    t_x = ((tgt[..., 0]*img_size) / stride.float() - grid_x.float()).clamp(epsilon, 1 - epsilon)\n",
    "    t_x = torch.log(t_x / (1. - t_x))   #inverse of sigmoid\n",
    "    t_y = ((tgt[..., 1]*img_size) / stride.float() - grid_y.float()).clamp(epsilon, 1 - epsilon)\n",
    "    t_y = torch.log(t_y / (1. - t_y))   # inverse of sigmoid\n",
    "\n",
    "    w_anchor = wh_anchor[..., 0] #/ (img_size + epsilon)\n",
    "    h_anchor = wh_anchor[..., 1] #/ (img_size + epsilon)\n",
    "    w_anchor = torch.index_select(w_anchor, 0, idx_anchor.view(-1)).view(idx_anchor.size())\n",
    "    h_anchor = torch.index_select(h_anchor, 0, idx_anchor.view(-1)).view(idx_anchor.size())\n",
    "    t_w = torch.log((tgt[..., 2] / w_anchor).clamp(min=epsilon))\n",
    "    t_h = torch.log((tgt[..., 3] / h_anchor).clamp(min=epsilon))\n",
    "\n",
    "    # Create the Processed Target Tensor:\n",
    "    # Updates the target tensor tgt_t with the local coordinates.\n",
    "    tgt_t = tgt.clone().detach()\n",
    "\n",
    "    tgt_t[..., 0] = t_x\n",
    "    tgt_t[..., 1] = t_y\n",
    "    tgt_t[..., 2] = t_w\n",
    "    tgt_t[..., 3] = t_h\n",
    "\n",
    "    # Aggregate Processed Targets and Indices:\n",
    "    n_batch = tgt.size(0)\n",
    "    n_pred = sum([(img_size // s) ** 2 for s in strides_selection]) * 3\n",
    "\n",
    "    idx_obj_1d = []\n",
    "    tgt_t_flat = []\n",
    "\n",
    "    for i_batch in range(n_batch):\n",
    "        v = idx_obj[i_batch]\n",
    "        t = tgt_t[i_batch]\n",
    "        l = tgt_len[i_batch]\n",
    "        idx_obj_1d.append(v[:l] + i_batch * n_pred)\n",
    "        tgt_t_flat.append(t[:l])\n",
    "\n",
    "    idx_obj_1d = torch.cat(idx_obj_1d)\n",
    "    tgt_t_flat = torch.cat(tgt_t_flat)\n",
    "\n",
    "    return tgt_t_flat, idx_obj_1d\n",
    "\n",
    "def iou_batch(bboxes1: Tensor, bboxes2: Tensor, center=False, zero_center=False):\n",
    "    x1 = bboxes1[..., 0]\n",
    "    y1 = bboxes1[..., 1]\n",
    "    w1 = bboxes1[..., 2]\n",
    "    h1 = bboxes1[..., 3]\n",
    "\n",
    "    x2 = bboxes2[..., 0]\n",
    "    y2 = bboxes2[..., 1]\n",
    "    w2 = bboxes2[..., 2]\n",
    "    h2 = bboxes2[..., 3]\n",
    "\n",
    "    area1 = w1 * h1\n",
    "    area2 = w2 * h2\n",
    "\n",
    "    epsilon = 1e-9\n",
    "\n",
    "    if zero_center:\n",
    "        w1.unsqueeze_(2)\n",
    "        w2.unsqueeze_(1)\n",
    "        h1.unsqueeze_(2)\n",
    "        h2.unsqueeze_(1)\n",
    "        w_intersect = torch.min(w1, w2).clamp(min=0)\n",
    "        h_intersect = torch.min(h1, h2).clamp(min=0)\n",
    "    else:\n",
    "        if center:\n",
    "            x1 = x1 - (w1 / 2)\n",
    "            y1 = y1 - (h1 / 2)\n",
    "            x2 = x2 - (w2 / 2)\n",
    "            y2 = y2 - (h2 / 2)\n",
    "        right1 = (x1 + w1).unsqueeze(2)\n",
    "        right2 = (x2 + w2).unsqueeze(1)\n",
    "        top1 = (y1 + h1).unsqueeze(2)\n",
    "        top2 = (y2 + h2).unsqueeze(1)\n",
    "        left1 = x1.unsqueeze(2)\n",
    "        left2 = x2.unsqueeze(1)\n",
    "        bottom1 = y1.unsqueeze(2)\n",
    "        bottom2 = y2.unsqueeze(1)\n",
    "        w_intersect = (torch.min(right1, right2) - torch.max(left1, left2)).clamp(min=0)\n",
    "        h_intersect = (torch.min(top1, top2) - torch.max(bottom1, bottom2)).clamp(min=0)\n",
    "\n",
    "    area_intersect = h_intersect * w_intersect\n",
    "    iou_ = area_intersect / (area1.unsqueeze(2) + area2.unsqueeze(1) - area_intersect + epsilon)\n",
    "    print(\"IoU:\", iou_)\n",
    "\n",
    "    return iou_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt = torch.tensor([[[2.5e-01, 2.5e-01, 5e-01, 5e-01, 1.0000e+00,\n",
    "          0.1000e+01],\n",
    "         [7.5e-01, 7.5e-01, 1.25e-01, 1.25e-01, 1.0000e+00,\n",
    "          0.2000e+01]],\n",
    "          ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = torch.randn(1, 3, 416, 416)  # Batch size of 1, 3 channels, 416x416 height and width\n",
    "\n",
    "# Pass the tensor through the model\n",
    "preds = model(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_bbox = preds[..., :4]\n",
    "print(f\"Preds min: {preds_bbox.min().item()}, max: {preds_bbox.max().item()}, mean: {preds_bbox.mean().item()}, std: {preds_bbox.std().item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, coord_loss, obj_loss, noobj_loss, class_loss = yolo_loss_fn(preds, tgt, [2], img_size=416)\n",
    "        \n",
    "print('train_loss:', loss)\n",
    "print('coord_loss:', coord_loss)\n",
    "print('obj_loss:', obj_loss)\n",
    "print('noobj_loss:', noobj_loss)\n",
    "print('class_loss:', class_loss)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
